{
  "files": [
    {
      "file_id": "uuid",
      "filename": "my.pdf",
      "content_type": "application/pdf",
      "text": "extracted text...",
      "keywords": [
        "capital",
        "france",
        "paris"
      ],
      "created_at": "2026-02-04T..."
    },
    {
      "file_id": "f908ce9d-26b2-439e-8856-8699614d595a",
      "filename": "1.pdf",
      "content_type": "application/pdf",
      "text": "1\nA Lightweight Supervised Intrusion Detection \nMechanism for IoT Networks \n \nSouradip Roy 1, Juan Li 1, Bong-Jin Choi 2, and Yan Bai 3 \n1Department of Computer Science, North Dakota State University \n2Department of Statistics, North Dakota State Univer sity \n3School Engineering and Technology, University of Wa shington Tacoma \n \nCorresponding Author:  \nJuan Li, PhD \nComputer Science Department, North Dakota State Uni versity \nFargo, ND, 58078, USA \nPhone: 1 701 232 9662 \nEmail: j.li@ndsu.edu \n \nAbstract — As the Internet of Things (IoT) is becoming incre asingly popular, we have experienced more security breaches that are \nassociated with the connection of vulnerable IoT de vices. Therefore, it is crucial to employ intrusion  detection techniques to mitigate \nattacks that exploit  IoT security vulnerabilities.   However, due to the limited capabilities of IoT d evices and the specific protocols used, \nconventional intrusion detection mechanisms may not  work well for IoT environments.  In this paper, we propose a novel intrusion \ndetection model that uses machine learning to effec tively detect cyber-attacks and anomalies in resour ce-constraint IoT networks. \nThrough a set of optimizations including removal of  multicollinearity, sampling, and dimensionality re duction, our model can identify \nthe most important features to detect intrusions us ing much fewer training data and less training time . Extensive experiments were \nperformed on the CICIDS2017 and NSL-KDD datasets re spectively to evaluate the proposed approach. The e xperimental results on two \npopular datasets show that our model has a high det ection rate and a low false alarm rate. It outperfo rms existing models in multiple \nperformance metrics and is consistent in classifyin g major cyber-attacks, respectively. Most important ly, unlike traditional \nresource-intensive intrusion detection systems, the  proposed model is lightweight and can be deployed on IoT nodes with limited power \nand storage capabilities. \n \nIndex Terms — Intrusion Detection, IoT network, machine learnin g, security \n \n1.  INTRODUCTION \nThe Internet of Things (IoT) connects uniquely iden tifiable heterogeneous embedded computing devices i n the physical \nenvironment to the Internet. It offers pervasive co nnectivity of devices, systems, and services, and h as been widely used in our daily \nlife. We have witnessed the explosion in connected devices and IoT technologies ranging from smart hom es, smart hospitals to drones \nand even autonomous bots. As more homes and busines ses adopt IoT devices, a large number of connected IoT devices will \nrevolutionize how data is processed and consumed.  IoT continues to enjoy an even greater surge in pop ularity, but on the other hand, \nthe associated security risks are also surging.  \nMany efforts have been applied to enhance IoT secur ity, including enforcement of encryption on data tr ansmitted in the network, \nstrict access control mechanisms for data confident iality, and various privacy and trust policies and management among users and IoT \ndevices. However, even with these mechanisms, IoT n etworks are still vulnerable to many kinds of cyber -attacks. Over the last few \nyears, there have been a tremendous number of IoT-c entric attacks: from more widescale, powerful distr ibuted denial of service \n(DDoS) attacks, to the hacking of baby monitors. In trusion Detection Systems (IDS) have, therefore, be en used as another layer of \ndefense in order to better protect the legitimate o peration of the IoT devices. \nIDS are devices or software applications that monit or network or system behaviors for malicious activi ties or policy violations and \nsend reports to a management station. Many IDS have  been proposed to improve Internet hosts and networ k security. However, we \ncannot deploy traditional IDS systems directly onto  IoT networks because of the special characteristic s of IoT networks: nodes in IoT \nnetworks are deployed in resource-constrained devic es, e.g., with limited power, computing, communicat ion, and storage capabilities. \nThis requires significant simplification, optimizat ion, and adaptation of existing security techniques . In addition, IoT network uses \ndifferent protocol stacks and standards. These nece ssities require to devise of security mechanisms ac cordingly. \nTo address the challenges faced by IDS in IoT netwo rks, in this paper, we propose a novel machine lear ning-based intrusion \ndetection mechanism, which has the following advant ages over traditional IDS systems: \n© 2021 published by Elsevier. This manuscript is made available under the Elsevier user license\nhttps://www.elsevier.com/open-access/userlicense/1.0/Version of Record: https://www.sciencedirect.com/science/article/pii/S0167739X21003733\nManuscript_3d1c8c455ce509a77a2bc911ec018c7f\n 2\n• Lightweight. Through a set of optimization mechanis ms including removal of multicollinearity, data sca ling, dimensionality \nreduction, and sampling, our mechanism can quickly identify the most important feature and dramaticall y reduce the size of the \nfeatures and data points needed for abnormality det ection, and consequently, lower computational compl exity. The lightweight \nfeature makes the algorithm appropriate for IoT net works with resource constraints. \n• Less training time. The proposed algorithm needs a much smaller training set. It, thus, dramatically r educes the training time \ncompared to the existing techniques.  \n• Higher detection rate for less popular cyber-attack s with infrequent observations, such as the User to  Root attack.  \n• Lower false positive and false negative rates. Our algorithm reduces the false positive rate while kee ping or improving the \naccuracy compared to the existing techniques. \nThe rest of the paper is organized as follows. Sect ion II surveys related work on various IDS for IoT networks and systems. Section \nIII describes our proposed methodology in detail. S ection IV presents our evaluation results. Finally,  in Section V, we provide \nconclusions and future work directions. \n2.  RELATED WORK  \nAccording to Zarpelão et al. [1], intrusion detecti on in IoT can be classified into four categories de pending upon the detection \nmechanism used in the system: signature-based, anom aly-based, specification-based, and hybrid. Signatu re-based and anomaly-based \nare the most widely used two approaches.  \nSignature-based IDS detects cyber-attacks the same way as virus scanners, by searching for specific pa tterns or signatures stored in \nthe IDS internal databases. If any system or networ k activity matches with stored patterns, it will be  detected as an intrusion. Sheikh et \nal. [2] proposed a lightweight signature-based IDS for IoT networks.  The IDS system consists of four parts including signature \ngenerator, pattern generator, intrusion detection e ngine, and output engine. They tested their system using the NSL-KDD dataset. Liu \net al. [3] proposed another signature-based IDS for  IoT. It uses an Artificial Immune System to detect  attacks. Attack signatures are \nstored in immune cells which can be further classif ied. The computation overhead of this approach is h igh. However, the authors did \nnot explain how this approach can be applied to res ource-limited IoT environments. Rebbah et al. [4]  proposed a signature-based IDS, \nnamed IoTSecurity, for IoT systems using Cloud. It calculates the temporary and spatial profile of eac h client based on the data of its \nrequest. Attacks are detected based on matching the  profile with the signature. While signature-based IDS is very efficient at detecting \nknown cyber-attacks, it is not effective in detecti ng new cyber-attacks. \nAnomaly-based IDS can identify an unknown activity by comparing it with a normal behavior profile and then classifying it as \neither normal or anomalous.  Anomaly-based IDS are different from signature-based IDS that only detect  attacks by matching \npreviously created signatures: Anomaly-based IDS ar e effective in identifying new intrusions. Many app roaches have been proposed \nto implement anomaly-based intrusion detection syst ems, among which deep learning has been widely used . Larijani et al. [5] \nproposed a random neural network-based intrusion de tection system (RNN-IDS) for IoTs. After selecting features, RNN-IDS trains \nand tests neurons at different learning rates with the NSL-KDD dataset. The performance is evaluated w ith the benchmark NSL-KDD \ndataset for binary classification. The system can o btain an accuracy of 94.50%. Similarly, Yin et al. [6] also proposed a deep learning \napproach that uses a recurrent neural network  (RNN ) for anomaly-based intrusion detection. Diro and C hilamkurti [7] applied deep \nlearning for attack detection in social IoT network s. Their experiments demonstrated that the deep mod el is more effective in attack \ndetection than its shallow counterparts. Alom et al . [8] used deep belief neural (DBN) networks to cre ate IDS and tested the system \nwith the NSL-KDD dataset. The proposed method achie ved a detection accuracy of about 97.5%.  Ahsan and  Nygard [9] proposed an \napproach using a hybrid algorithm of Convolutional Neural Network (CNN) and Long Short Term Memory (LS TM). Ieracitano et al. \n[10] proposed a statistical analysis-driven optimiz ed deep learning system for intrusion detection. Th e system extracts features using \nbig data visualization and statistical analysis met hods, followed by a deep autoencoder (AE) for poten tial threat detection.  \nDifferent strategies have been proposed to reduce t he false prediction rate and improves accuracy.  Fo r example, Song et al. [11] \npropose a multiple decision-based classification me thod to identify misclassified data, and classify t hem into three different classes, \ncalled a malicious, benign, and ambiguous dataset. They evaluated their approach with the recent real- world network traffic data, \nKyoto2006+ datasets. Khan et al. [12] proposed a hy brid-multilevel anomaly prediction approach to deal  with unbalanced intrusion \ndata to improve the accuracy of the detection syste m.  \nMany machine learning-based algorithms have been ap plied for IDS based on the CICIDS2017 dataset [13].  A deep neural network \n(DNN)-based IDS is proposed to detect and classify cyberattacks [14]. Researchers have performed a com prehensive evaluation of \nDNNs and other classical machine learning classifie rs using different datasets including KDDCup 99, NS L-KDD [15], UNSW-NB15 \n[16], Kyoto [17], WSN-DS [18], and CICIDS2017 [13].  They confirmed that DNNs perform well in compariso n with the classical \nmachine learning classifiers. Sethi et al. proposed  a reinforcement learning-based IDS that employs De ep Q-Network logic in multiple \ndistributed agents and uses attention mechanisms to  classify network attacks [19] . They also tested their data using NSL-KDD and \nCICIDS2017 and demonstrated improved performance. Y ulianto et al. apply a set of mechanisms including oversampling,  Principal \nComponent Analysis (PCA), and Ensemble Feature Sele ction (EFS) to improve the performance of AdaBoost- based IDS on CICIDS \n2017 Dataset [20].  Another work also tries to solv e the data imbalance problem by using the Generativ e Adversarial Networks (GAN) \nmodel [21]. It uses a deep learning approach to add ress data imbalances and Random Forest to classify attacks.  \n 3\nOther machine learning algorithms were also explore d for IDS based on the NSL-KDD dataset. Li et al. [ 15] proposed K-Nearest \nNeighbor (KNN) classification algorithm in wireless  sensor networks for intrusion detection. Shapoorif ard and Shamsinejad [22] \nworked on combining the KNN classifier with K-MEANS  clustering for intrusion detection. Ingre et al. [ 23] proposed a Decision \nTree-based IDS and tested it with the NSL-KDD datas et. Farnaaz et al. [24] used Random Forest for dete ction anomaly. As specified \nby Sasha, “Anomaly-based Intrusion Detection at bot h the network and host levels have a few shortcomin gs; namely a \nhigh false-positive rate and the ability to be fool ed by a correctly delivered attack” [25].   \n3.  METHODOLOGY  \nThe goal of this research is to construct an effici ent classifier model from limited information and r esources to detect various \nattacks as accurately as possible. This is achieved  by a series of mechanisms including (A) removal of  multicollinearity, which also \nreduces the dimensionality of data; (B) sampling da ta to reduce the required training data size and im prove detection rate; (C) \nfurther dimensionality removal to reduce overfittin g, and (D) effective classification algorithm. In t he following subsections, we \npresent the details of these four mechanisms. \n3.1 Data Segmentation and Feature Ranking \nFeature ranking is the process of assigning a score  to each of the independent variables. The informat ion contained in these \nindependent variables helps the model to correctly predict the class of the samples. This ranking meth od helps us to determine the \nmost important features in the dataset. This techni que will help add non-linearity to our reduced data set which is described in \nSection 3.3. To correctly identify the important fe atures and rank them based on their importance we h ave used regression analysis. \nA regression model may cause problems to correctly interpret the important features if multicollineari ty exists within the feature \nset. Multicollinearity is a condition when there is  a significant dependency or association between th e independent variables or the \npredictor variables. Multicollinearity may misleadi ngly inflate the standard error of a data model in an excessive amount [26]. In \nthe presence of multicollinearity, the coefficient may provide high estimates of changes in the multip le regressions when only \nsmall changes can be seen in the model or the data.  Therefore, it is important to detect and remove mu lticollinearity. We use the \nVariance Inflation Factor (VIF) [27] to identify mu lticollinearity. As defined in Equation (1), VIF me asures multicollinearity \namong the independent variables in a feature set by  using multiple regression analysis.  \n\u0001\u0002\u0003 =  1\n1 − \b \t\n                                     (1)  \nIn equation (1), R i2, termed as r-squared, is the regression of the i th  independent variable from the feature set. r-squar ed is calculated \nas: \n\b\r2= 1 −\u000f(\u0010\r− \u0011)\n\u000f(\u0010\r− \u0012\u0013)                          (2)  \nin which Y i is the actual class of the data point and µ is the  predicted value by the logistic regression model. The numerator in the \nfraction is known as sum squared regression and the  denominator is called sum squared total. The sum s quared regression is the \npartial model where only the independent variable i s considered. VIF indicates how much the variance ( behavior) of an \nindependent variable is influenced by its interacti on with others. If there is a significant presence of multicollinearity then the VIF \nof the independent variable will be significantly h igh. Once the VIF for all the features is obtained,  it is important to determine a \nthreshold value, above which all the features will be called multicollinearity. \nTraditionally there are two ways to remove multicol linearity: one is to drop features with high VIF; t he other is to apply the \nPartial Least Squares Regression (PLS) [28] or Prin cipal Component Analysis (PCA) [29] to all features  above a threshold to \nmerge them to one feature [26]. Both methods may ca use information loss because of removing features d irectly. We try to keep as \nmuch as possible information on the original datase t to avoid information loss. For this purpose, we u se data segmentation that will \ngenerate new features. These new features represent  the original features of the dataset without multi collinearity. We creatively \npropose to group variables (with VIF beyond the thr eshold) into n groups according to their VIF. In ou r experiment, we set n=5. \nThe 5 groups include least multicollinearity, mediu m multicollinearity, moderate multicollinearity, hi gh multicollinearity, and very \nhigh multicollinearity. We then apply PCA to repres ent features in a group in a lower dimension. PCA i s a mathematical algorithm \nthat reduces data dimensionality while retaining mo st of the variation in the data set. This is achiev ed by identifying directions (i.e., \nprincipal components), along which the variation in  the data is maximal.  \nIn order to decide on the number of principal compo nents needed for each group, we have used a scree p lot [30]. This plotting \ntechnique helps us to identify the minimum number o f principal components needed to retain the require d percentage of variance in \nthe data. In our experiment, we choose the number o f principal components for each group to be the min imum number to achieve \n90% of the variance explained. The goal is to maxim ize the percentage of variance while keeping the sm allest number of principal \ncomponents. The 90% of variance defines most of the  information contained by the principal component c ompared to the original \nfeatures. Once the required number of principal com ponents for each group is obtained, we use the prin cipal components as an \nindependent feature. The linear combination of all the data points obtained from the principal compone nts is treated as new \nobservations. We dropped all features having VIF va lues higher than the collinearity threshold t from the original dataset and \n 4\nappended these newly generated features. Using this  approach, in our experiment on the NSL-KDD data se t, after removing \nmulticollinearity from the dataset, we reduced the features from 41 to 28. After the removal of multic ollinearity, we used a \nregression model to rank the features and understan d the importance of each of the independent variabl es to define the class of the \nsamples. \nWe apply data normalization to normalize the range of independent variables. The floor and ceiling val ues of the data points for \neach feature may not fall in the same range and the  distribution of the data is unknown, using standar dization allows us to rescale \nthe features which have the properties of a standar d normal distribution. We use z-score [27] normaliz ation that is defined in \nEquation 3: \n\u0014 =\u0015\t− \u0016̅\n\u0018\u0019                                     (3)  \nin which X i is the individual value belonging to an observatio n of a feature, \u0016̅ is the mean value of each feature and S x is the \nstandard deviation. Z-score allows the data points to follow a normal distribution curve and ensures e ach feature’s data distribution \nhas a mean value of 0 and a standard deviation of 1 . If the dataset has unextreme outliers, using scal ing, we do not need to remove \nthem. \n3.2. Sampling \nDue to the nature of imbalanced network attacks [31 ], the occurrences of some cyber-attacks are destin ed to be much more \nfrequent than some others. Therefore, the dataset u sed to develop the classification model is often un balanced, i.e., classes are \nunevenly distributed. For example, compared with ot her attacks, the number of User to Root attacks is much smaller in our \nexperimental dataset. Class imbalance can give rise  to a series of problems, such as a biased model to wards predicting the majority \nclass, misclassification of observations, overfitti ng of the model, etc. \nOne of our goals is to reduce the training time yet  be effective in detecting anomalies. We use sampli ng techniques to select \nindividuals to draw statistical inferences about th e population. Specifically, we apply undersampling on the majority classes and \noversampling on the minority ones. Random undersamp ling with replacement is used where a sample is sel ected from the dataset in \nwhich every data has equal chances of being selecte d. The probability of each sample being selected fr om the population is 1/N \nwhere N is the size of the population.  \nThe oversampling method we have used is SMOTE [32].  It uses the concept of nearest neighbors to genera te synthetic data for a \nminority class. The new synthetic data are generate d between existing minority instances. This method takes three parameters to \ngenerate new instances which are T, N, k where T is  the number of minority class samples, N is the per centage explaining the \nminority class to be over-sampled and k is the numb er of nearest neighbors that need to be considered for the oversampling \ntechnique. \nUnder- and oversampling may cause the cross-class n earest neighbors' problem, a complication in which a single data point may \nhave equidistant neighbors falling in different cat egories. When this happens, it is difficult to plac e the data point to its correct \nclass. Therefore, it is important to eliminate thes e data points; otherwise, they can cause misclassif ication. We use Tomek Link [33] \nto clean up overlap between classes. A Tomek Link i s defined as follows: given two samples s i, and s j, d(s i, s j) is the distance \nbetween s i and s j.  si, s j is called a Tomek link if there are no other sample s s o, such that d(s i, s o)< d(s i, s j) or d(s j, s o)< d(s i, s j). When \ntwo samples form a Tomek link, either one of these samples is noise or both of them are close to the c lass border. Removing such \nsamples helps us to create well-defined classes, an d consequently improving classification performance . \n3.3. Dimensionality Reduction and Addition of Non-L inearity \n     After removing multicollinearity and sampling, we r educe the number of features and samples used in th e dataset. These \napproaches dramatically reduce the computation comp lexity and help fast response in detecting attacks in real-time.  However, as the \nnumber of observations gets reduced, the sample den sity of the dataset will decrease. The result of de creased sample density and \nsparsity in the dataset will cause a model overfitt ing problem, i.e., corresponding too closely to the  training data, but fail to fit \nadditional data or predict the future. To solve thi s problem, dimensionality reduction is applied. A d imensionality reduction is a \nstatistical procedure where a set of independent va riables are projected to a lower dimension using a set of principal variables. We \nutilize the PCA approach that uses unsupervised lea rning to detect hidden patterns within the data.  A ssume that we have a dataset M \nwith dimension N:  \n\u001b =  \u001c\u0015\u001d\u001d ⋯ \u0015 \u001d\u001f\n⋮ ⋱ ⋮\n\u0015\u001f\u001d ⋯ \u0015 \u001f\u001f \"                     (4) \nM is a N×N dimensional Matrix, in which  \n• Xij  are the data points where 1 ≤ i ≤ N and 1 ≤ j ≤ N. \n• Each column in M is an independent variable F k where 1 ≤ k ≤ N \nThe covariance score between F k of M can be calculated as \n 5\n#$% (\u0015, \u0010 )=  1\n' − 1\u000f(\u0015( − \u0015 \u0013)(\u0010( − \u0010 \u0013))    (5)    \nwhere \u0015, \u0010 ∈ \u0003, \u0015 0 \u0010, \u0015( ∈ \u0015, \u0010( ∈ \u0010 . The resulting matrix is the covariance matrix A. \n1 =  \u001c\u001b\u001d\u001d ⋯ \u001b\u001d\u001f\n⋮ ⋱ ⋮\n\u001b\u001f\u001d ⋯ \u001b \u001f\u001f \"                    (6) \n \nin which M ij  represents the covariance score between each indep endent variable. \nAfter obtaining the covariance matrix A, we need to  find the eigenvalues. Each eigenvalue is considere d as a principal \ncomponent. The eigenvalues are used to determine th e eigenvector of the matrix. We determine the eigen value by the following \nequation: \ndet (1 − 4\u0002 )= 0                                            (7)  \n\u0002 = \u001c1 ⋯ 0\n⋮ ⋱ ⋮\n0 ⋯ 1\"                                             (8) \nI is the identity matrix of the same dimension as a covariance matrix. Then I is multiplied with scalar λ, and the result matrix is 4\u0002 : \n4\u0002 = \u001c4 ⋯ 0\n⋮ ⋱ ⋮\n0 ⋯ 4\"                                     (9)  \n \nThe result of the determinant of equation 7 will pr ovide an n-degree polynomial in terms of λ: \n84ⁿ : ;4ⁿ<\u001d: ⋯ : # = 0                           (10 )  \nSolving this equation gives us a scalar of 1×n eige nvalues, and these eigenvalues can be used to produ ce eigenvectors. Each of \nthe values is treated as a principal component in t he following order: the first eigenvalue is the fir st principal component, the second \neigenvalue is the second principal component, and s o on. After we obtain all of the eigenvalues, we us e the following equation to \nproduce the eigenvector corresponding to each eigen value.  \n(1 − 4\u0002 )\u0016 = 0                                             (11 ) \nThe process is generally known as the Gaussian Elim ination process [34]. The number of eigenvectors wi ll be equal to the number \nof eigenvalues. Each eigenvector will contain the s ame number of data points as the original matrix M.  Once we have all the \neigenvectors, they are sorted in decreasing order. Afterward, k vectors are chosen to form an N × k di mensional matrix. This new \nmatrix will serve as the dataset in the process of model training and testing. \nAgain, we use a scree plot to determine the number of principal components required for dimensionality  reduction. Fig. 1 shows \nthe percentage of variance explained by our NSL-KDD  experimental dataset with respect to the number of  principal components. \nAs shown in Fig. 1, 10 principal components explain  almost 95% of the variance in the original variabl es. Further adding principal \ncomponents would not increase variance percentage m uch. Therefore, we choose 10 principal components. It has obtained the best \nresults considering the least number of principal c omponents. The dataset obtained after using PCA is linear. It is obvious that any \nkind of model used for classifying a linear dataset  will always have satisfying results. To prove that  our model is equally \ntrustworthy in classifying a non-linear dataset, we  added three independent variables from the origina l dataset as features. The three \nfeatures have the least variance influential factor  which is added as features besides the principal c omponents. We conducted \nextensive experiments on adding the different numbe r of independent variables from the original datase t besides the principal \ncomponents. The experimental results help us to ide ntify that three is the best and least number of in dependent variables which can \nbe used, as adding more than three independent vari ables doesn’t improve the classifier’s performance  \n \nFig. 1.  Number of Principal Components defining th e percentage of explained variance by the dataset \n\n 6\n3.4.  Ensemble Learning \n     As presented in Section II Related Work, vario us machine learning approaches have been proposed t o classify network \nintrusions. We propose B-Stacking, a new approach o f stacking based on an adaptive combination of boos ting and stacking \nalgorithms. Boosting and stacking are ensemble lear ning methods that are normally used separately. A s tacking model involves \ntwo or more base models, often referred to as level -0 models, and a meta-model that combines the predi ctions of the base models \nreferred to as a level-1 model. The level-0 classif iers pass on probabilities or predicted values as f eatures to the level-0 classifier, \nwhich then uses these values and the original obser vation vector to make the final classification.   \nOur B-Stacking algorithm chooses several simple cla ssifiers as the level-0 weak learners. Among the we ak learners, one of them \nshould be a boosting algorithm, an ensemble meta-al gorithm for primarily reducing bias and variance. F or instance, in our \nexperiments, we choose K-Nearest Neighbors, Random Forest, and XGBoost [35] as the level-0 learners. X GBoost is a booting \nlearning algorithm. In XGBoost, weights are the sum  of gradients scaled by the sum of Hessians.  Choos ing a boosting learner in \nlevel-0 aims to reduce the bias of the feature set on which the level-1 learner will be trained. In ad dition, a stacking ensemble model \nhelps to minimize the variance in the data. Therefo re, combining the stacking and boosting at level-0 will create a generalized \nintrusion detection system that can perform unbiase d classification across all the classes.  Boosting is also used as the level-1 \nlearner in the B-Stacking algorithm with the purpos e of reducing the bias of the learning model, as th e level-1 boosting learner \nmakes the final prediction. In summary, level-0 boo sting helps to minimize the bias of the dataset, wh ile level-1 learner boosting \nmakes the architecture of the model generalized eno ugh to fit various datasets. Therefore, combining b oosting and stacking \nalgorithms in both level-0 and level-1 brings us mu ltiple benefits. Another advantage of the B-stackin g algorithm is that the \nboosting leaner can be trained parallelly to reduce  computation time.   \nThe advantage of our B-Staking classifier is that i t does not require a large amount of training data and training time, as what the \nNeural Network requires. But the voting process of each learner gives the model the advantages of mult iple decision models. The \nbasic procedure for the proposed algorithm is summa rized in Algorithm 1.  \n4.   EVALUATIONS  \n4.1.  Datasets and Data Processing \nWe tested our approach on two popular and publicly available datasets, CICIDS2017 [13]. NSL-KDD [33]. CICIDS2017 is one of \nthe most recent IDS datasets that contains benign a nd some common attack network flows, which meet rea l-world criteria and are \npublicly available. It is widely used in the most r ecent studies of cybersecurity for real-world intru sion detection. This dataset also \ncontains network traffic analysis results obtained by using CICFlowMeter. It contains 79 features for each record, with 78 of the \nfeatures refer to the network traffic and the remai ning one defines whether it is a particular kind of  intrusion or normal. There are 14 \ndifferent intrusion detection classes present in th e dataset. We represent the record count of each cl ass in Fig. 2. As can be seen from \nFig. 2, five classes including Infiltration, Web At tack Brute Force, Web Attack XSS, Web Attack SQL In jection and Heartbleed have \nalmost no records compared to other classes of the dataset. We combined these five classes and represe nt them as a single class. In \naddition, we find that DDoS, PortScan, and DoS Hulk  have significantly more records than other classes . Therefore, we apply \ndifferent sampling techniques in sequential order m entioned in Section 3.2 to create a balanced datase t. After combining the five \nclasses in CICIDS2017 we have 11 classes where 0 re presents normal traffic and all the anomaly classes  are represented by each \nnumber ranging from 1 to 10.  We have removed some records with missing values.  \n Algorithm 1:   B-Sacking Algorithm  \n/*   Input: Sample data of N × k (N is the number o f    observation, and k is \nthe number of features) */ \n/*Output: Trained B-stacking Classifier  */ \n \n/*Training of Base Classifiers. */ \n1.  Initiate a Stacked Ensemble Model S \n2.  Assign Weak classifiers L to S’s lower level \n3.  Set the Boosting model B to S’s top level \n4.  For i=1 to N \n5.         train L on k features \n6.         feed output of L to input of B \n7.         extracts n features where n ≠k using B \n8.  End For \n/*Training of Meta-Classifier. */ \n9.  Pass n features to meta classifier M  \n10.  For j=1 to N \n11.        train M on n features \n12.  End For \n13.  End \n 7\nBecause of its popularity, we also used NSL-KDD as another dataset to evaluate our system. The NSL-KDD  data set was a revised, \ncleaned-up version of the KDD’99 dataset [36] from the University of New Brunswick Canadian Institute for Cybersecurity. These \ndata sets contain the records of the Internet traff ic with ghosts of the traffic encountered by real I DS. The data set contains 43 features \nper record, with 41 of the features referring to th e traffic input itself and the last two are labels as normal traffic or malicious traffic and \nScore indicating the severity of the traffic input itself. The 41 features include 7 discrete and 34 c ontinuous variables. There are 39 \ndifferent types of cyber-attacks in the training an d testing dataset which can be categorized into 5 m ajor classes, and they are termed as \nNormal, Denial of Services (DoS), Root to Local (R2 L), User to Root (U2R), and Probe. We classify the four major types of attack \ncategories for the NSL-KDD dataset. We define class  labels as multivariate where each class is represe nted by a number ranging from \n0 to 4 (the value 0 represents normal traffic and o ther values represent an attack type).  \nFig. 3 shows the setup procedure of the experimenta l model. We applied the procedures presented in Sec tion III on the \nCICIDS2017 datasets to execute our methodology. The  figure also shows the size of the data after each operation.  We applied the \nsame procedure to the NSL-KDD dataset.  \n \n \n \n \n \n \n \n \n \n \n \n \n \nFig. 3 Experimental Model Setup CICIDS2017 Dataset \nData Segmentation 2450532 * 78  \nDimensionality Addition of Non-linearity \nfrom Original Dataset 24020 * 24  \n24020 * 15 24020 * 3  \nB-Stacking Classifier \nBase Learners (K Neighbors, Random Forest, XGBoost)  \nMeta-Classifier (XG-Boost) \nClassified Intrusions  \nFig.2 Record Count of Each Class in CICIDS2017 \n  020000 40000 60000 80000 100000 120000 140000 160000 180000 200000 \n 8\n4.2.  Performance Metrics \n• In order to evaluate the proposed detection mechani sm, we have compared our mechanism with other state s of the arts. Our \nperformance metrics, such as Recall, Precision, Acc uracy, and AUC-ROC Curve, are defined based on the confusion matrix. The \nconfusion matrix is a summary of prediction results  on a classification problem. The number of correct  and incorrect predictions \nare summarized with count values and broken down by  each class.  From the confusion matrix, we get the  following derivations: \nTrue Positive (TP), False Positive (FP), True Negat ive (TN), and False Negative (FN). Base on the confusion matrix, we have \nused the following metrics to evaluate our proposed  mechanism: Accuracy, Area Under the Curve (AUC): T rue Positive Rate \n(TPR), False Positive Rate (FPR),Precision, Recall,  F1-Score, and Precision-Recall Curve (PRC).   \nThese metrics were used for binary classification a nd do not natively support our classification task with more than two classes. We \napply the One-vs-Rest strategy [37] to use these bi nary classification algorithms for the multi-classi fication problem.  The basic idea \nof this strategy is to split the multi-class classi fication dataset into multiple binary classificatio n datasets and fit a binary classification \nmodel on each. \n4.3.  Result and Discussion  \nTo better understand the performance of our propose d intrusion detection mechanism, we compare its per formance with the state of \nthe arts in terms of the metrics presented in the p revious section. All these systems used the same CI CIDS2017 or NSL-KDD datasets. \n4.3.1. Evaluation of the CICIDS2017 dataset \nFig. 4 shows the confusion matrix of our model for the CICIDS2017 dataset. We had a 7:3 split on our s ample data of the \nsize 24,020, i.e., 70% for training and 30% for tes ting. The diagonal elements in Fig. represent the n umber of instances from each \nclass that has been correctly classified by our alg orithm in this dataset. The total number of observa tions in our test data are 597, \n704, 639, 650, 651, 666, 684, 660, 661, 661, and 63 3 for Benign, DDoS, PortScan, Bot, the combined cla ss (Infiltration, Web \nAttack Brute Force, Web Attack XSS, Web Attack SQL Injection and Heartbleed), FTP-Patator, SSH-Patator , DoS \nSlowloris, DoS Slowhttptest, DoS Hulk, DoS GolenEye  respectively. As shown in the confusion matrix, th e false-positive rates for \neach of the classes in the CICIDS2017 dataset are v ery low which should be the main purpose of an anom aly-based \nIDS.  Our B-Stacking algorithm only misclassified 6 0 samples among 7,206 in the test data. It has a ve ry high detection rate in \nclassifying the anomaly classes correctly.   \nBased on the confusion matrix, we plot the ROC curv e and Precision-Recall curve using our detection mo del. ROC Curves \nsummarize the trade-off between the TPR and FPR for  a predictive model using different probability thr esholds. While \nPrecision-Recall curves summarize the trade-off bet ween the TPR and the positive predictive value for a predictive model using \ndifferent probability thresholds.  We used the  One-vs-Rest  strategy  to extend the precision-recall curve to our multi-c lass \nproblem.  Specifically,  we converted the output for each class to  binary and use those outputs to draw one curve  for each  label. The \nsame process was executed for producing the ROC cur ve for each class.   As illustrated in Fig.5 and Fig. 6, both curves co ver 100% \nor near 100% of the area while detecting the classe s. We evaluated the performance of our model on eac h class independently. The \nmicro-average value of the ROC and Precision-Recall  curve that our model shows excellent performance.     \n \nFig. 4. Confusion matrix of our detection model on CICIDS2017 dataset \n  \n\n 9\nTo further understand the performance of our propos ed detection model, we compare its performance with  state-of-the-art \ntechniques including DNN [14], A-DQN [19], Adaboost  [20], and GAN [21]. In Table I, we examined the pe rformance of our \nB-Stacking algorithm in terms of accuracy, precisio n, recall, and F1 score with the four other represe ntative intrusion detection \nsystems that have been explained in Section II Rela ted Work. As can be seen from the table that our B- stacking algorithm achieves \ngood performance on all four metrics.   It is compa rable to the state-of-the-art. Moreover, as can be seen in the latter part of this \nsection, we have compared the overhead of our B-sta cking model with other algorithms, and it incurs mu ch less overhead compared \nwith others, such as DNN_kNN [38], which is a hybri d model comprising of a neural network and k-neares t neighbor.   \n \nTABLE  I \nPERFORMANCE COMPARISON ON CICIDS2017  DATASET  \n \nMODEL NAME   ACCURACY (%)   PRECISION (%)   RECALL (%)   F1-SCORE (%)   \nDNN   95.6  96.2  95.6  95.7  \nGAN   99.83  98.69  92.76  95.04  \nA-DQN   98.7  98.6  99.4  98.9  \nADABOOST   91.83  91.15  100  90.01  \nB-STACKING  99.11  99.08  99.11  99.08  \n \nWe also studied the system overhead in terms of mem ory overhead and CPU overhead on the CICIDS2017 dat aset. The \ncomputing node which has been used in performing th e analysis of memory overhead and CPU overhead for the B-stacking model \nis an Intel®Core TM i5-9400F CPU 2.90 GHz ∗ 6 notebook with 8GB of RAM. Fig. 7 plots the memor y overhead of the \nB-Stacking model while performing the classificatio n task of the CICIDS2017 dataset. It can be noticed  from the figure that during \nthe 60 seconds evaluation time of packet flows to d etect anomalies, our model used a constant memory o f 3.4% of 8GB RAM. \nCompared with a recently proposed lightweight IDS s ystem [38] that used a similar testing environment,  they allocate 10% of the \nmemory before their DNN-kNN algorithm started the a nalysis. During the analysis phase, DNN-kNN occupie d around 15% of the \nmemory which determines that around 5% of the memor y is required by DNN-kNN to operate. Therefore, our  system performs \nsimilarly to theirs in terms of memory overhead. \n \n \nFig. 7 Memory overhead on CICIDS2017 dataset \n \n      Fig. 5. ROC curve of each class on CICIDS2017  dataset                                      Fig. 6. Precision-recall curve of each class on CICIDS20 17 dataset \n \n\n 10  \n \n \n Fig. 8 CPU overhead on CICIDS2017 dataset \n \nFig. 8 shows the CPU consumption for processing the  network traffics. It can be noticed that B-stackin g used CPU with a range \nof 1.5% to 2.9%. As a comparison, the CPU usage for  the DNN-kNN model [38] was around 12%.  We conclud e that compared to \nDNN-kNN, our model is extremely lightweight and can  be easily used in fog nodes or IoT nodes where the  computational resource \nis limited.  \n \n4.3.2. Evaluation of the NSL-KDD dataset \nSimilar experiments have been performed on the NSL- KDD dataset. Fig. 9 and 10 plot the ROC curve and P recision-Recall curve \nusing our detection model. Again, both curves cover  100% or near 100% of the area while detecting the classes. We compared \nB-stacking with other state-of-the-art techniques. In Table II, we examine the accuracy of our B-Stack ing algorithm with 8 other \nrepresentative intrusion detection systems that hav e been explained in Section II Related Work. The ac curacy value of our model \nexceeds most of these approaches, except CNN-LSTM ( Ahsan and Nygard, [9]) and kFN-KNN (Shapoorifard et  al., [12]). However, \nresearchers of kFN-KNN reported accuracy alone with out any other metrics in their paper. This is probl ematic as NSL-KDD is an \nimbalanced dataset, high accuracy does not mean the  system can classify minority classes well. Moreove r, they classified the dataset \nas a binary classification. Our model, instead, is a multi-class classifier that can track the specifi c type of attacks and allows users to \ntake measures accordingly. Similarly, although CNN- LSTM slightly outperformed our model in accuracy, t his model cannot identify \nthe probe attack at all. It is also possible that t heir test data did not contain any observations fal ling into the Probe class. Therefore, it is \nnot clear about the performance of the model regard ing the classification of all types of attacks. Thi s can be demonstrated by the \nfollowing tests shown in Fig. 11-13. \nFigs. 11-13 compare our B-Stacking model with 5 oth er IDS, namely AE (Ieracitano et al., [10]), CNN-LS TM (Ahsan and Nygard, \n[9]), RNN (Yin et al., [6]), Shallow Model, and Dee p Model (Diro and Chilamkurti, [7]). As kFN-KNN (Sh apoorifard et al., [12]), \nCFS-DT (Ingre et al., [13]), and DBN (Alom et al., [8]) only provides accuracy and we cannot find thes e metrics for comparison). \nFrom these figures, we can see as Deep Model and Sh allow Model combined the Root to Local and User to Root class as one class and \nboth models have not performed very well in classif ying the different attack types. RNN model also per formed similarly and there lies \nan inconsistency in detecting each of the attack ty pes.  \n \nFig. 9. ROC curve of each class on the NSL-KDD data set                                           Fig. 10. Precision-recall curve of each class on the NSL -KDD dataset \n \n\n 11  \nComparing with these models our B-Stacking algorith m is consistent across all the classes and can clas sify most of the instances  \nFig. 12. Comparison of Recall on the NSL-KDD datase t \n  0 10 20 30 40 50 60 70 80 90 100 Deep Model Shallow Model RNN CNN-LSTM AE B-Stacking \nProbe User to Root Root to Local DoS Normal TABLE  II \nCOMPARISON OF ACCURACY ON THE NSL-KDD  DATASET  \nModels Accuracy (%) \nDeep Model (Diro and Chilamkurti, [7]) 98.27 \nShallow Model ( Diro  and Chilamkurti , [7]) 96.75  \nkFN -KNN ( Shapoorifard et al.,  [12 ]) 99  \nCFS-DT (Ingre et al., [13]) 90.3 \nDBN (Alom et al. , [8]) 97.5  \nRNN (Yin et. al .,  [6]) 83.28  \nCNN-LSTM (Ahsan and Nygard, [9]) 99 \nAE (Ieracitano et al., [10]) 87 \nB-Stacking (Our Algorithm)  98.5 ± 0.3  \n  \n \n \nFig. 11. Comparison of Precision on the NSL-KDD dat aset \n  0 10 20 30 40 50 60 70 80 90 100 Deep Model Shallow Model RNN CNN-LSTM AE B-Stacking \nProbe User to Root Root to Local DoS Normal \n 12  \ncorrectly. Our model involves a sample set of 10,90 4 instances which is 0.07% of the whole dataset. Th e use of a stacking classifier \nhelps our meta-classifier to train on the most impo rtant features in the whole dataset. Since we used 3 base learners as the process of \nfeature extraction, our final meta-classifier is tr ained on 3 features. These improvements make the B- Stacking model very lightweight \nand take much less amount of time in terms of train ing and prediction. The total time taken to train t he meta-classifier after extracting \nfeatures from the base-classifier is 0.28 seconds a nd the prediction time is 0.02 seconds. The deep le arning-based approaches \nincluding CNN-LSTM, RNN, AE, Deep Model, and Shallo w model are time-consuming procedures, as deep neur al networks take \nmuch more computation time when a large number of r ecords are involved. \n5.  CONCLUSION  \nOne of the most important technological progress ov er the past decade was the widespread adoption of I oT devices across \nindustries. Improving the security of IoT infrastru cture is crucial to ensure the security and safety of industries and societies. In this \npaper, we present a novel IoT intrusion detection m odel B-Stacking, which uses optimized machine learn ing approaches to effectively \ndetect cyber-attacks in an IoT network. We have per formed extensive experiments to evaluate the perfor mance of our system. The \nresult shows that B-Stacking has a high detection r ate and a very low false alarm rate. It overperform s most of the state of the art \ntechniques.  In the future, we plan to test our mod el with different IoT datasets and apply it to a re al IoT network. \nFUNDING : \nThis work was supported in part by the National Sci ence Foundation (NSF) with award numbers: 1722913 a nd 1921576. \nREFERENCES  \n \n[1] B. B. Zarpelão, R. S. Miani, C. T. Kawakani, an d S. C. de Alvarenga, “A survey of intrusion detect ion in Internet of Things,” Journal of Network and \nComputer Applications . 2017, doi: 10.1016/j.jnca.2017.02.009. \n[2] N. U. Sheikh, H. Rahman, S. Vikram, and H. AlQa htani, “A Lightweight Signature-Based IDS for IoT E nvironment,” arXiv Prepr. arXiv1811.04582 , \n2018. \n[3] R. T. Liu, C. H. Chen, C. N. Kao, and N. F. Hua ng, “A Fast String-Matching Algorithm for Network P rocessor-Based Intrusion Detection System,” ACM \nTrans. Embed. Comput. Syst. , 2004, doi: 10.1145/1015047.1015055. \n[4] M. Rebbah, D. E. H. Rebbah, and O. Smail, “Intr usion detection in Cloud Internet of Things environ ment,” in 2017 International Conference on \nMathematics and Information Technology (ICMIT) , 2017, pp. 65–70. \n[5] H. Larijani, J. Ahmad, N. Mtetwa, and others, “ A novel random neural network based approach for in trusion detection systems,” in 2018 10th Computer \nScience and Electronic Engineering (CEEC) , 2018, pp. 50–55. \n[6] C. Yin, Y. Zhu, J. Fei, and X. He, “A Deep Lear ning Approach for Intrusion Detection Using Recurre nt Neural Networks,” IEEE Access , 2017, doi: \n10.1109/ACCESS.2017.2762418. \n[7] A. A. Diro and N. Chilamkurti, “Distributed att ack detection scheme using deep learning approach f or Internet of Things,” Futur. Gener. Comput. Syst. , \n2018, doi: 10.1016/j.future.2017.08.043. \n[8] M. Z. Alom, V. Bontupalli, and T. M. Taha, “Int rusion detection using deep belief networks,” 2016,  doi: 10.1109/NAECON.2015.7443094. \n[9] M. Ahsan and K. Nygard, “Convolutional Neural N etworks with LSTM for Intrusion Detection,” Proc. 35th Int. Confer , vol. 69, pp. 69–79, 2020. \n[10] C. Ieracitano et al. , “Statistical Analysis Driven Optimized Deep Learn ing System for Intrusion Detection,” 2018, doi: 10. 1007/978-3-030-00563-4_74. \n[11] C. Song, W. Fan, S. Y. Chang, and Y. Park, “Re constructing Classification to Enhance Machine-Lear ning Based Network Intrusion Detection by  \nFig. 13. Comparison of F1 on the NSL-KDD dataset \n  0 20 40 60 80 100 Deep Model Shallow Model RNN CNN-LSTM AE B-Stacking \nProbe User to Root Root to Local DoS Normal \n 13  \nEmbracing Ambiguity,” 2021, doi: 10.1007/978-3-030- 72725-3_13. \n[12] I. A. Khan, D. Pi, Z. U. Khan, Y. Hussain, and  A. Nawaz, “Hml-ids: A hybrid-multilevel anomaly pr ediction approach for intrusion detection in scada \nsystems,” IEEE Access , 2019, doi: 10.1109/ACCESS.2019.2925838. \n[13] A. Boukhamla and J. C. Gaviro, “CICIDS2017 Dat aset: Performance Improvements and Validation as a Robust Intrusion Detection System Testbed,” Int. \nJ. Inf. Comput. Secur. , 2018. \n[14] R. Vinayakumar, M. Alazab, K. P. Soman, P. Poo rnachandran, A. Al-Nemrat, and S. Venkatraman, “Dee p Learning Approach for Intelligent Intrusion \nDetection System,” IEEE Access , 2019, doi: 10.1109/ACCESS.2019.2895334. \n[15] W. Li, P. Yi, Y. Wu, L. Pan, and J. Li, “A new  intrusion detection system based on KNN classifica tion algorithm in wireless sensor network,” J. Electr. \nComput. Eng. , 2014, doi: 10.1155/2014/240217. \n[16] N. Moustafa and J. Slay, “UNSW-NB15: A compreh ensive data set for network intrusion detection sys tems (UNSW-NB15 network data set),” 2015, doi: \n10.1109/MilCIS.2015.7348942. \n[17] J. Song, H. Takakura, Y. Okabe, M. Eto, D. Ino ue, and K. Nakao, “Statistical analysis of honeypot  data and building of Kyoto 2006+ dataset for NIDS \nevaluation,” 2011, doi: 10.1145/1978672.1978676. \n[18] I. Almomani, B. Al-Kasasbeh, and M. Al-Akhras,  “WSN-DS: A Dataset for Intrusion Detection Systems  in Wireless Sensor Networks,” J. Sensors , 2016, \ndoi: 10.1155/2016/4731953. \n[19] K. Sethi, Y. V. Madhav, R. Kumar, and P. Bera,  “Attention based multi-agent intrusion detection s ystems using reinforcement learning,” J. Inf. Secur. \nAppl. , 2021, doi: 10.1016/j.jisa.2021.102923. \n[20] A. Yulianto, P. Sukarno, and N. A. Suwastika, “Improving adaboost-based intrusion detection syste m (IDS) performance on CIC IDS 2017 dataset,” in \nJournal of Physics: Conference Series , 2019, vol. 1192, no. 1, p. 12018. \n[21] J. H. Lee and K. H. Park, “GAN-based imbalance d data intrusion detection system,” Pers. Ubiquitous Comput. , 2021, doi: 10.1007/s00779-019-01332-y. \n[22] H. Shapoorifard and P. Shamsinejad, “Intrusion  Detection using a Novel Hybrid Method Incorporatin g an Improved KNN,” Int. J. Comput. Appl. , 2017, \ndoi: 10.5120/ijca2017914340. \n[23] B. Ingre, A. Yadav, and A. K. Soni, “Decision tree based intrusion detection system for NSL-KDD d ataset,” in International Conference on Information \nand Communication Technology for Intelligent System s , 2017, pp. 207–218. \n[24] N. Farnaaz and M. A. Jabbar, “Random Forest Mo deling for Network Intrusion Detection System,” 201 6, doi: 10.1016/j.procs.2016.06.047. \n[25] B. Sasha, “A strict anomaly detection model fo r IDS,” Phrack Mag. Vol. 0xa Issue 0x38, May1 , vol. 138, 2000. \n[26] K. P. Vatcheva and M. Lee, “Multicollinearity in Regression Analyses Conducted in Epidemiologic S tudies,” Epidemiol. Open Access , 2016, doi: \n10.4172/2161-1165.1000227. \n[27] R. M. O’Brien, “A caution regarding rules of t humb for variance inflation factors,” Qual. Quant. , 2007, doi: 10.1007/s11135-006-9018-6. \n[28] J. M. Andrade-Garda, A. Carlosena-Zubieta, R. Boqué-Marti, and J. Ferré-Baldrich, “Partial least- squares regression,” in RSC Analytical Spectroscopy \nSeries , 2013. \n[29] H. Abdi and L. J. Williams, “Principal compone nt analysis,” Wiley Interdisciplinary Reviews: Computational Stat istics . 2010, doi: 10.1002/wics.101. \n[30] M. Zhu and A. Ghodsi, “Automatic dimensionalit y selection from the scree plot via the use of prof ile likelihood,” Comput. Stat. Data Anal. , 2006, doi: \n10.1016/j.csda.2005.09.010. \n[31] D. A. Cieslak, N. V. Chawla, and A. Striegel, “Combating imbalance in network intrusion datasets, ” 2006, doi: 10.1109/grc.2006.1635905. \n[32] R. Blagus and L. Lusa, “SMOTE for high-dimensi onal class-imbalanced data,” BMC Bioinformatics , 2013, doi: 10.1186/1471-2105-14-106. \n[33] E. AT, A. M, A.-M. F, and S. M, “Classificatio n of Imbalance Data using Tomek Link (T-Link) Combi ned with Random Under-sampling (RUS) as a \nData Reduction Method,” Glob. J. Technol. Optim. , 2016, doi: 10.4172/2229-8711.s1111. \n[34] J. F. Grcar, “How ordinary elimination became Gaussian elimination,” Hist. Math. , 2011, doi: 10.1016/j.hm.2010.06.003. \n[35] T. Chen and C. Guestrin, “XGBoost: A scalable tree boosting system,” 2016, doi: 10.1145/2939672.2 939785. \n[36] M. Tavallaee, E. Bagheri, W. Lu, and A. A. Gho rbani, “A detailed analysis of the KDD CUP 99 data set,” 2009, doi: 10.1109/CISDA.2009.5356528. \n[37] “Pattern Recognition and Machine Learning,” J. Electron. Imaging , 2007, doi: 10.1117/1.2819119. \n[38] C. A. de Souza, C. B. Westphall, R. B. Machado , J. B. M. Sobral, and G. dos S. Vieira, “Hybrid ap proach to intrusion detection in fog-based IoT \nenvironments,” Comput. Networks , 2020, doi: 10.1016/j.comnet.2020.107417.",
      "keywords": [
        "lightweight",
        "supervised",
        "intrusion",
        "detection",
        "mechanism",
        "for",
        "iot",
        "networks",
        "souradip",
        "roy",
        "juan",
        "bong",
        "jin",
        "choi",
        "and",
        "yan",
        "bai",
        "1department",
        "computer",
        "science",
        "north",
        "dakota",
        "state",
        "university",
        "2department",
        "statistics",
        "univer",
        "sity",
        "3school",
        "engineering",
        "technology",
        "shington",
        "tacoma",
        "corresponding",
        "author",
        "phd",
        "department",
        "uni",
        "versity",
        "fargo",
        "58078",
        "usa",
        "phone",
        "701",
        "232",
        "9662",
        "email",
        "ndsu",
        "edu",
        "abstract",
        "the",
        "internet",
        "things",
        "becoming",
        "incre",
        "asingly",
        "popular",
        "have",
        "experienced",
        "more",
        "security",
        "breaches",
        "that",
        "are",
        "associated",
        "with",
        "connection",
        "vulnerable",
        "vices",
        "therefore",
        "crucial",
        "employ",
        "techniques",
        "mitigate",
        "attacks",
        "exploit",
        "vulnerabilities",
        "however",
        "due",
        "limited",
        "capabilities",
        "evices",
        "specific",
        "protocols",
        "used",
        "conventional",
        "mechanisms",
        "may",
        "not",
        "work",
        "well",
        "environments",
        "this",
        "paper",
        "propose",
        "novel",
        "model",
        "uses",
        "machine",
        "learning",
        "effec",
        "tively",
        "detect",
        "cyber",
        "anomalies",
        "resour",
        "constraint",
        "through",
        "set",
        "optimizations",
        "including",
        "removal",
        "multicollinearity",
        "sampling",
        "dimensionality",
        "duction",
        "our",
        "can",
        "identify",
        "most",
        "important",
        "features",
        "intrusions",
        "ing",
        "much",
        "fewer",
        "training",
        "data",
        "less",
        "time",
        "extensive",
        "experiments",
        "were",
        "performed",
        "cicids2017",
        "nsl",
        "kdd",
        "datasets",
        "spectively",
        "evaluate",
        "proposed",
        "approach",
        "xperimental",
        "results",
        "two",
        "show",
        "has",
        "high",
        "det",
        "ection",
        "rate",
        "low",
        "false",
        "alarm",
        "outperfo",
        "rms",
        "existing",
        "models",
        "multiple",
        "performance",
        "metrics",
        "consistent",
        "classifyin",
        "major",
        "respectively",
        "unlike",
        "traditional",
        "resource",
        "intensive",
        "systems",
        "deployed",
        "nodes",
        "power",
        "storage",
        "index",
        "terms",
        "network",
        "learnin",
        "introduction",
        "connects",
        "uniquely",
        "iden",
        "tifiable",
        "heterogeneous",
        "embedded",
        "computing",
        "devices",
        "physical",
        "environment",
        "offers",
        "pervasive",
        "nnectivity",
        "services",
        "been",
        "widely",
        "daily",
        "life",
        "witnessed",
        "explosion",
        "connected",
        "technologies",
        "ranging",
        "from",
        "smart",
        "hom",
        "hospitals",
        "drones",
        "even",
        "autonomous",
        "bots",
        "homes",
        "busines",
        "ses",
        "adopt",
        "large",
        "number",
        "will",
        "revolutionize",
        "how",
        "processed",
        "consumed",
        "continues",
        "enjoy",
        "greater",
        "surge",
        "pop",
        "ularity",
        "but",
        "other",
        "hand",
        "risks",
        "also",
        "surging",
        "many",
        "efforts",
        "applied",
        "enhance",
        "secur",
        "ity",
        "enforcement",
        "encryption",
        "ansmitted",
        "strict",
        "access",
        "control",
        "confident",
        "iality",
        "various",
        "privacy",
        "trust",
        "policies",
        "management",
        "among",
        "users",
        "these",
        "etworks",
        "still",
        "kinds",
        "over",
        "last",
        "few",
        "years",
        "there",
        "tremendous",
        "entric",
        "widescale",
        "powerful",
        "distr",
        "ibuted",
        "denial",
        "service",
        "ddos",
        "hacking",
        "baby",
        "monitors",
        "trusion",
        "ids",
        "another",
        "layer",
        "defense",
        "order",
        "better",
        "protect",
        "legitimate",
        "peration",
        "software",
        "applications",
        "monit",
        "system",
        "behaviors",
        "malicious",
        "activi",
        "ties",
        "policy",
        "violations",
        "send",
        "reports",
        "station",
        "improve",
        "hosts",
        "networ",
        "cannot",
        "deploy",
        "directly",
        "onto",
        "because",
        "special",
        "characteristic",
        "constrained",
        "devic",
        "communicat",
        "ion",
        "requires",
        "significant",
        "simplification",
        "optimizat",
        "adaptation",
        "addition",
        "different",
        "protocol",
        "stacks",
        "standards",
        "nece",
        "ssities",
        "require",
        "devise",
        "cordingly",
        "address",
        "challenges",
        "faced",
        "netwo",
        "rks",
        "lear",
        "ning",
        "based",
        "which",
        "following",
        "advant",
        "ages",
        "2021",
        "published",
        "elsevier",
        "manuscript",
        "made",
        "available",
        "under",
        "user",
        "license",
        "https",
        "www",
        "com",
        "open",
        "userlicense",
        "version",
        "record",
        "sciencedirect",
        "article",
        "pii",
        "s0167739x21003733",
        "3d1c8c455ce509a77a2bc911ec018c7f",
        "optimization",
        "mechanis",
        "sca",
        "ling",
        "reduction",
        "quickly",
        "feature",
        "dramaticall",
        "reduce",
        "size",
        "points",
        "needed",
        "abnormality",
        "consequently",
        "lower",
        "computational",
        "compl",
        "exity",
        "makes",
        "algorithm",
        "appropriate",
        "net",
        "works",
        "constraints",
        "needs",
        "smaller",
        "thus",
        "dramatically",
        "educes",
        "compared",
        "higher",
        "attack",
        "infrequent",
        "observations",
        "such",
        "root",
        "positive",
        "negative",
        "rates",
        "reduces",
        "while",
        "kee",
        "ping",
        "improving",
        "accuracy",
        "rest",
        "organized",
        "follows",
        "sect",
        "surveys",
        "related",
        "section",
        "iii",
        "describes",
        "methodology",
        "detail",
        "presents",
        "evaluation",
        "finally",
        "provide",
        "conclusions",
        "future",
        "directions",
        "according",
        "zarpel",
        "detecti",
        "classified",
        "into",
        "four",
        "categories",
        "pending",
        "upon",
        "signature",
        "anom",
        "aly",
        "specification",
        "hybrid",
        "signatu",
        "anomaly",
        "approaches",
        "detects",
        "same",
        "way",
        "virus",
        "scanners",
        "searching",
        "tterns",
        "signatures",
        "stored",
        "internal",
        "databases",
        "any",
        "activity",
        "matches",
        "patterns",
        "detected",
        "sheikh",
        "consists",
        "parts",
        "generator",
        "pattern",
        "ngine",
        "output",
        "engine",
        "they",
        "tested",
        "their",
        "using",
        "dataset",
        "liu",
        "artificial",
        "immune",
        "cells",
        "further",
        "classif",
        "ied",
        "computation",
        "overhead",
        "igh",
        "authors",
        "did",
        "explain",
        "res",
        "ource",
        "rebbah",
        "named",
        "iotsecurity",
        "cloud",
        "calculates",
        "temporary",
        "spatial",
        "profile",
        "eac",
        "client",
        "its",
        "request",
        "matching",
        "very",
        "efficient",
        "detecting"
      ],
      "created_at": "2026-02-04T05:56:40.113237Z"
    }
  ]
}